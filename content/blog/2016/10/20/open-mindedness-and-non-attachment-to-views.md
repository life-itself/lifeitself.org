---
title: Open-mindedness and Non-attachment to Views
created: 2016-10-20
categories:
  - institute
tags:
  - non-attachment
  - open-minded
authors:
  - rufuspollock
image: /assets/attachment-detachment-nonattachment.png
---

> *… Seeing that harmful actions arise from anger, fear, greed, and intolerance, which in turn come from dualistic and discriminative thinking, I will cultivate openness, non-discrimination, and non-attachment to views in order to transform violence, fanaticism, and dogmatism in myself and in the world.*
> 
> **Thich Nhat Hanh, First Mindfulness Training “Reverence for Life”**[^1]

> *The human understanding when it has once adopted an opinion draws all things else to support and agree with it. And though there be a greater number and weight of instances to be found on the other side, yet these it either neglects and despises, or else by some distinction sets aside and rejects, in order that by this great and pernicious predetermination the authority of its former conclusion may remain inviolate.*
> 
> **Bacon, 1620, Novum Organum Aphorism 46**

> *When the facts change, I change my mind. What do you do, sir!*
> 
> **John Maynard Keynes, Economist, (Apocryphal)**

[^1]: http://plumvillage.org/mindfulness-practice/the-5-mindfulness-trainings/

Consider that one of the greatest obstacles to our well-being is our attachment to views — our desire to believe that we are right and others are wrong. As a result, we find it hard to change our views and to hear others. This can result in dogmatism, fanaticism and violence – both physical and emotional. At a personal level, our attachment to views may be one of the greatest obstacle to our own well-being and enlightenment because of the difficulty we face relinquishing deep-rooted beliefs in an inherently existing 'self' or 'I'.[^2]

[^2]:  See the definition of “ontological addiction” in Shonin, E., Van Gordon, W., & Griffiths, M. D. (2013a). Buddhist philosophy for the treatment of problem gambling. Journal of Behavioral Addictions, 2, 63–71.

In this primer we look at three areas related to this issue. First, we look at what we mean by attachment and non-attachment to views and their relationship to engagement and detachment. We clarify that non-attachment is not **de**attachment but rather un-attached engagement and open-mindedness.

Second, we present scientific evidence on attachment to views: that we resist changing our views despite strong, contrary evidence. We discuss various reasons why that might be and the impact of this materially and spiritually.

Third, we explore the deeper connections with ontology and our ideas of self in philosophy and Buddhism.

## What are Attachment and Non-Attachment?

> [!Summary]
> 
> **Simply put, non-attachment to views = openminded-ness. Openminded-ness that is deep, engaged and constructively critical**
> 
> **More broadly, non-attachment is neither attachment nor detachment. It is a space of engaged, seriously light-hearted commitment**

![[assets/attachment-detachment-nonattachment.png]]

Ordinarily we think of attachment as something positive or even neutral: I'm attached to this old watch because my father gave it to me, or the boat is attached to the shore by a rope. And conversely to be unattached sounds a bit negative. For example, if you say “I'm unattached” it means you are without a romantic relationship — whereas to be attached is to have one (observe that common slang for getting married is to “get hitched” which roughly approximates to to “get attached”).

And this sense is still there when it comes to views. Not to be attached to a view is often called _de_\-tached. Whilst this can be a positive sense of dispassionate and independent as in “the judge considered the arguments with a sense of detachment”, there is also the sense of being uninvolved and uncaring: "the man watched the dogs attack the fox with an air of detachment".

Thus our use of attachment may be surprising. In ordinary english attachment is often used in as positive context: we are attached to places, people and things that we like and care about. Conversely, the opposite of attachment — *de*tachment — has a mildly negative sense of emotionless unconcern, anomie, lifelessness — “he kissed her with an air of detachment”, “he lived detached, absent, as if something were permanently missing”.

Our use of attachment and non-attachment derives from a Buddhist tradition. In that tradition “attachment” is the translation for key concept around the way that we “cling” to things: experiences, things, ideas, even consciousness. It can be found as a key phrase in translations of the Four Noble Truths, the core teachings of the Buddha:[^3]

[^3]: See e.g. http://www.buddhaweb.org/

1. Life involves suffering
2. Suffering arises from _attachment_ \[also translated as “craving”, “clinging to” …\]
3. Well-being is possible -- suffering ceases when _attachment_ ceases
4. Wellbeing and freedom from suffering is possible by practicing the Eightfold Path

The special usage also explains why we use \*non-\*attachment rather than _de_\-tachment as the contrast to attachment. Non-attachment, which is our focus here, is not detachment. It is not simply an absence, a lack of attachment. Rather it is something positive, a positive choice that makes true engagement and commitment possible.

Consider an analogy with listening. When we listen to another person we can listen in several ways. One way to listen is to do so passively. It is listening just as not talking but without really engaging with what the person is saying. What they say comes in our ears but we do not really hear it or listen to it in a true sense. This is “detached” listening. On the other hand, there are times when we truly listen and listen deeply. This is an active not a passive act. We actively engage ourselves with what they are saying, opening our mind to it, positively welcoming what they are saying.

## Science of Attachment in Everyday Life

If we look around us: at newspapers, at our friends, even in our own lives, it becomes clear that misinformation is ubiquitous, and that false and erroneous beliefs are widespread and persistent.

For example, over half of all voters in Republican Primaries in 2011 were "birthers" who believed that President Obama was not born in the United States despite overwhelming evidence to the contrary -- ranging from a Hawaiian birth certificate to announcements in local papers that his pregnant mother went into the Honolulu hospital and left it cradling a baby.

On a less political note, a sizeable minority in Britain and the United States believe in a link between vaccination and autism. This is thanks to a single 1998 study in the prestigious Lancet journal. Widely reported, the study was subsequently completely discredited — it was based on a tiny sample of just twelve cases and its result have never been replicated in larger trials; there was strong evidence of fraud in the paper’s analysis which led to the Lancet formally retracting the paper and the British General Medical Council striking off the study’s author for malpractice. This has had a large impact for public health as parents refuse to vaccinate their children resulting in a significant number of preventable deaths and illness.[^a]

[^a]: A summary of the evidence that the 1998 paper was defective and fraudulent can be found in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3136032 which links to most of the other papers on the topic. For numbers see e.g. http://www.jennymccarthybodycount.com/. "For example, following the unsubstantiated claims of a vaccination-autism link, many parents decided not to immunize their children, which has had dire consequences for both individuals and societies, including a marked increase in vaccine-preventable disease and hence preventable hospitalizations, deaths, and the unnecessary expenditure of large amounts of money for follow-up research and public-information campaigns aimed at rectifying the situ- ation (Larson et al., 2011; Poland & Spier, 2010; Ratzan, 2010)." [Lewandowsky et al 2012, p.107)

Other famous examples are denial of evidence for evolution and for global warming.

The famous phenomenon of [cognitive dissonance](https://en.wikipedia.org/wiki/Cognitive_dissonance), is consistent with ancient beliefs about attachment to views. Psycholigists define Cognitive Dissonance as the negative feeling of mental stress that one feels when confronted with two contradictory beliefs, or engaging in behavior that conflicts with beliefs. In a review of studies goal pursuit, [a team of psychologists lead by William Hart](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4797953/) found that the more beliefs are important for a goal (i.e. the more we are attached to them), the more we tend to experience dissonance that causes us to avoid information that contradicts these beliefs, and tend minimize or discount information that contradicts our positions, even when we cannot avoid it. In comparison, subjects show minimal avoidance of information that conflicted with beliefs that were not important for the maintenance of a goal (i.e. beliefs to which participants were not attached.) This shows that attachment to a particular goal can distort our reality, and so we should be mindful of our attachment at all times.

A recent New Yorker article, _[I don't want to be right](https://www.newyorker.com/science/maria-konnikova/i-dont-want-to-be-right)_ chronicles the attempt of a team of scientists to take psychology out of the lab and change deeply held, but wrong real-world beliefs. These scientists designed interventions to change views on “birtherism” and the belief that autism is caused by vaccines, which has its origins in a single retracted study. Their efforts to change beliefs have been largely unsuccessful, overall what they found was that “if information doesn't square with someone's prior beliefs, he discards the beliefs if they're weak and discards the information if the beliefs are strong.” When beliefs that are central to a person's self-concept or which are the basis for important decisions, such as whether to have their children immunized, are challenged they will tend to resolve to resolve the resulting discomfort by discounting or undermining the challenging information.

The lone intervention that makes a (small) difference is having participants engage in self-affirmation, recounting positive moments from their past. This increased faith in one's self helps to buffer subjects from conflict between the threat to their self concept that comes from being told that they might “be wrong” about something.

Buddhist teachings would suggest not to be attached to being right in the first place. This non-attachemnt is cultivated through the practice of being mindful of one's attachments. These include not only the unpleasant sensations that come from being challenged, also the emotional attachments, such as pride and righteousness, that are the ultimate source of our attachment.

## Buddhism, ontology and the "self"

Note that as mentioned above, with regards to detachment, this does not imply that one should not care about truth. We are so used to depending on attachment to pride in being smart or morally superior to justify our attempts understand the world, that we can't imagine any other way. It is a contention of Buddhism that such a way exists. 

This may sound extraordinary, but it is not. Just think about the last time that you learned something new, that you had no ego involvement in. You were probably interested, but were also able to change your mind from one moment to the next. Non-attachment is simply inhabiting such a state all of the time.

This is done by simply acknowledging our feelings but not be “caught-up in them” – not giving them great importance. This helps to maintain a calm mind, essential to reasoning clearly, and changing one's mind when the facts change. This is part of common mindfulness practice, which helps to calm emotions and a racing mind.

Science in its relatively short engagement with Buddhist philosophy has not tested this notion thoroughly, but published reviews of the mindfulness literature suggest links between the extinction of attachment by mindfulness training and well-known phenomenon, such as extinction of fear by exposure conditioning.[^4]

[^4]: [How Does Mindfulness Meditation Work? Proposing Mechanisms of Action From a Conceptual and Neural Perspective](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.477.2270&rep=rep1&type=pdf), and [Perspectives on Psychological Science , 2011 6: 537, Britta K. Hölzel, Sara W. Lazar, Tim Gard, Zev Schuman-Olivier, David R. Vago and Ulrich Ott](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.477.2270&rep=rep1&type=pdf)


## References

- I Don't Want to Be Right - New Yorker _[http://www.newyorker.com/science/maria-konnikova/i-dont-want-to-be-right](https://www.newyorker.com/science/maria-konnikova/i-dont-want-to-be-right)_
- Memory for Fact, Fiction, and Misinformation: The Iraq War 2003 Psychological Science March 2005 16: 190-195
- Nyhan, Brendan, Jason Reifler, and Peter A. Ubel. 2013. 'The Hazards of Correcting Myths About Health Care Reform': _Medical Care_ 51 (2): 127–32. doi:10.1097/MLR.0b013e318279486b. http://journals.lww.com/lww-medicalcare/Abstract/2013/02000/The\_Hazards\_of\_Correcting\_Myths\_About\_Health\_Care.2.aspx
  > Context: Misperceptions are a major problem in debates about health care reform and other controversial health issues. Methods: We conducted an experiment to determine if more aggressive media fact-checking could correct the false belief that the Affordable Care Act would create “death panels.” Participants from an opt-in Internet panel were randomly assigned to either a control group in which they read an article on Sarah Palin’s claims about “death panels” or an intervention group in which the article also contained corrective information refuting Palin. Findings: The correction reduced belief in death panels and strong opposition to the reform bill among those who view Palin unfavorably and those who view her favorably but have low political knowledge. However, it backfired among politically knowledgeable Palin supporters, who were more likely to believe in death panels and to strongly oppose reform if they received the correction. Conclusions: These results underscore the difficulty of reducing misperceptions about health care reform among individuals with the motivation and sophistication to reject corrective information.
- Misinformation and Its Correction: Continued Influence and Successful Debiasing Psychological Science in the Public Interest December 2012 13: 106-131, _[http://psi.sagepub.com/content/13/3/106.full](http://psi.sagepub.com/content/13/3/106.full)_
- Lord C, Lepper MR, Ross L. Biased assimilation and attitude polarization. The effects of prior theories on subsequently considered evidence. _Journal of Personality and Social Psychology_ 1979; 37: 2098-2110. _[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.372.1743&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.372.1743&rep=rep1&type=pdf)_
  > People who hold strong opinions on complex social issues are likely to examine relevant empirical evidence in a biased manner. They are apt to accept "con- firming" evidence at face value while subjecting "discontinuing" evidence to critical evaluation, and as a result to draw undue support for their initial posi- tions from mixed or random empirical findings. Thus, the result of exposing contending factions in a social dispute to an identical body of relevant em- pirical evidence may be not a narrowing of disagreement but rather an in- crease in polarization. To test these assumptions and predictions, subjects supporting and opposing capital punishment were exposed to two purported studies, one seemingly confirming and one seemingly disconfirming their exist- ing beliefs about the deterrent efficacy of the death penalty. As predicted, both proponents and opponents of capital punishment rated those results and procedures that confirmed their own beliefs to be the more convincing and probative ones, and they reported corresponding shifts in their beliefs as the various results and procedures were presented. The net effect of such evaluations and opinion shifts was the postulated increase in attitude polarization.
- Hart, P. Sol, and Erik C. Nisbet. 2012. 'Boomerang Effects in Science Communication How Motivated Reasoning and Identity Cues Amplify Opinion Polarization About Climate Mitigation Policies'. Communication Research 39 (6): 701–23. doi:10.1177⁄0093650211416646.  _[http://crx.sagepub.com/content/39/6/701](http://crx.sagepub.com/content/39/6/701)_
  
  > The deficit-model of science communication assumes increased communication about science issues will move public opinion toward the scientific consensus. However, in the case of climate change, public polarization about the issue has increased in recent years, not diminished. In this study, we draw from theories of motivated reasoning, social identity, and persuasion to examine how science-based messages may increase public polarization on controversial science issues such as climate change. Exposing 240 adults to simulated news stories about possible climate change health impacts on different groups, we found the influence of identification with potential victims was contingent on participants’ political partisanship. This partisanship increased the degree of political polarization on support for climate mitigation policies and resulted in a boomerang effect among Republican participants. Implications for understanding the role of motivated reasoning within the context of science communication are discussed. 
- Fishing a Superfund Site: Dissonance and Risk Perception of Environmental Hazards by Fishermen in Puerto Rico_[http://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.1991.tb00603.x/full](http://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.1991.tb00603.x/full)_
  
  > Risk perception studies show that individuals tend to underestimate significant risks, overestimate negligible ones, and distrust authorities. They also rely on a variety of strategies or heuristics to reach decisions regarding their risk-taking behavior. We report on a survey of fishermen and crabbers engaged in recreational and subsistence fishing in a Puerto Rican estuary (near Humacao), which has been declared a “Superfund site” because of suspected contamination by mercury, and at ecologically similar control sites. Nearly everyone interviewed at the Humacao site was aware of the mercury contamination, but either denied its importance, believed the contamination was restricted to a distant part of the estuary, or assumed that the estuary would be closed by the authorities if the threat was real. All site-users consumed the fish and crabs they caught

- Garrett, R. Kelly, and Brian E. Weeks. 2013. 'The Promise and Peril of Real-Time Corrections to Political Misperceptions'. In _Proceedings of the 2013 Conference on Computer Supported Cooperative Work_, 1047–1058. CSCW '13. New York, NY, USA: ACM. doi:10.1145⁄2441776.2441895. _[http://dl.acm.org/citation.cfm?id=2441895](https://dl.acm.org/citation.cfm?id=2441895)_
  
  > Computer scientists have responded to the high prevalence of inaccurate political information online by creating systems that identify and flag false claims. Warning users of inaccurate information as it is displayed has obvious appeal, but it also poses risk. Compared to post-exposure corrections, real-time corrections may cause users to be more resistant to factual information. This paper presents an experiment comparing the effects of real-time corrections to corrections that are presented after a short distractor task. Although real-time corrections are modestly more effective than delayed corrections overall, closer inspection reveals that this is only true among individuals predisposed to reject the false claim. In contrast, individuals whose attitudes are supported by the inaccurate information distrust the source more when corrections are presented in real time, yielding beliefs comparable to those never exposed to a correction. We find no evidence of real-time corrections encouraging counterargument. Strategies for reducing these biases are discussed.

- Lewandowsky, Stephan, Michael E. Mann, Nicholas J. L. Brown, and Harris Friedman. 2016. 'Science and the Public: Debate, Denial, and Skepticism'. _Journal of Social and Political Psychology_ 4 (2): 537–53. doi:10.5964/jspp.v4i2.604.

General Cognitive Mechanisms

- Belief in the Law of Small Numbers. AMOS TVERSKY and DANIEL KAHNEMAN. Hebrew University of Jerusalem. Psychological Bulletin, 1971, Vol. 76, No. 2. 105-110.
  
  > Abstract - People have erroneous intuitions about the laws of chance. In particular, they regard a sample randomly drawn from a population as highly representative, that is, similar to the population in all essential characteristics. The prevalence of the belief and its unfortunate consequences for psychological research are illustrated by the responses of professional psychologists to a questionnaire concerning research decisions.**

Buddhism and Ontology

- The Emerging Role of Buddhism in Clinical Psychology: Toward Effective Integration [PDF](https://www.apa.org/pubs/journals/features/rel-a0035859.pdf)

## Related Posts

- [Jonathan Ekstrom: Non Attachment to Views](https://lifeitself.org/2016/10/06/non-attachment-to-views-by-jonathan-ekstrom/)
- [Uncommon Wisdom, Gautama – Introduction](https://lifeitself.org/2016/10/23/my-introduction-to-gautamas-uncommon-wisdom/)

## Appendix: A Loving Father Rejects His Son

An old story attributed to the historical "Buddha", Siddhartha Gautama, communicates the Buddhist perspective elegantly (Recounted in _The Art of Power_ by Thich Nhat Hanh, pp.87-89)

The Buddha tells the story of a merchant, a widower, who went away in a business trip and left his little boy at home. While he was away, bandits came and burned down the whole village. When the merchant returned, he didn't find his house, it was just a heap of ash. There was the charred body of a child close by. He threw himself on the ground and cried and cried. He beat his chest and pulled his hair. The next day, he had the little body cremated. Because his beloved son was his only reason for existence, he sewed a beautiful velvet bag and put the ashes inside. Wherever he went, he took that bag of ashes with him. Eating, sleeping, working, he always carried it with him.

In fact, his son had been kidnapped by the bandits. Three months later, the boy escaped and returned home. When he arrived, it was two o'clock in the morning. He knocked on the door of the new house his father had built. The poor father was lying on his bed crying, holding the bag of ashes, and he asked, 'Who is there?' 'It's me, Daddy, your son.' The father answered, 'That's not possible. My son is dead. I've cremated his body and I carry his ashes with me. You must be some naughty boy who's trying to fool me. Go away, don't disturb me!' He refused to open the door, and there was no way for the little boy to come in. The boy had to go away, and the father lost his son forever.

After telling the story, the Buddha said, 'If at some point in your life you adopt an idea or a perception as the absolute truth, you close the door of your mind. This is the end of seeking the truth. And not only do you no longer seek the truth, but even if the truth comes in person and knocks on your door, you refuse to open it. Attachment to views, attachment to ideas, attachment to perceptions are the biggest obstacle to the truth.'

## Appendix: Excerpts

### A real world impact of "attachment" 

Lewandowsky et al. 2012, pp. 106-107:

> On August 4, 1961, a young woman gave birth to a healthy baby boy in a hospital at 1611 Bingham St., Honolulu. That child, Barack Obama, later became the 44th president of the United States. Notwithstanding the incontrovertible evidence for the simple fact of his American birth—from a Hawaiian birth certificate to birth announcements in local papers to the fact that his pregnant mother went into the Honolulu hospital and left it cradling a baby—a group known as “birthers” claimed Obama had been born outside the United States and was therefore not eligible to assume the presidency. Even though the claims were met with skepticism by the media, polls at the time showed that they were widely believed by a sizable proportion of the public (Travis, 2010), including a majority of voters in Republican primary elections in 2011 (Barr, 2011).
> 
> In the United Kingdom, a 1998 study suggesting a link between a common childhood vaccine and autism generated considerable fear in the general public concerning the safety of the vaccine. The UK Department of Health and several other health organizations immediately pointed to the lack of evidence for such claims and urged parents not to reject the vaccine. The media subsequently widely reported that none of the original claims had been substantiated. Nonetheless, in 2002, between 20% and 25% of the public continued to believe in the vaccine- autism link, and a further 39% to 53% continued to believe there was equal evidence on both sides of the debate (Hargreaves, Lewis, & Speers, 2003). More worryingly still, a substantial number of health professionals continued to believe the unsub- stantiated claims (Petrovic, Roberts, & Ramsay, 2001). Ulti- mately, it emerged that the first author of the study had failed to disclose a significant conflict of interest; thereafter, most of the coauthors distanced themselves from the study, the journal offi- cially retracted the article, and the first author was eventually found guilty of misconduct and lost his license to practice medi- cine (Colgrove & Bayer, 2005; Larson, Cooper, Eskola, Katz, & Ratzan, 2011).

### It is not just the facts, it is the narrative we tell about the facts

From "I don’t want to be right":

> Even when we think we’ve properly corrected a false belief, the original exposure often continues to influence our memory and thoughts. In a series of studies, Lewandowsky and his colleagues at the University of Western Australia asked university students to read the report of a liquor robbery that had ostensibly taken place in Australia’s Northern Territory. Everyone read the same report, but in some cases racial information about the perpetrators was included and in others it wasn’t. In one scenario, the students were led to believe that the suspects were Caucasian, and in another that they were Aboriginal. At the end of the report, the racial information either was or wasn’t retracted. Participants were then asked to take part in an unrelated computer task for half an hour. After that, they were asked a number of factual questions (“What sort of car was found abandoned?”) and inference questions (“Who do you think the attackers were?”). After the students answered all of the questions, they were given a scale to assess their racial attitudes toward Aboriginals.
> 
> Everyone’s memory worked correctly: the students could all recall the details of the crime and could report precisely what information was or wasn’t retracted. But the students who scored highest on racial prejudice continued to rely on the racial misinformation that identified the perpetrators as Aboriginals, even though they knew it had been corrected. They answered the factual questions accurately, stating that the information about race was false, and yet they still relied on race in their inference responses, saying that the attackers were likely Aboriginal or that the store owner likely had trouble understanding them because they were Aboriginal. This was, in other words, a laboratory case of the very dynamic that Nyhan identified: strongly held beliefs continued to influence judgment, despite correction attempts—even with a supposedly conscious awareness of what was happening.
> 
> In a follow-up, Lewandowsky presented a scenario that was similar to the original experiment, except now, the Aboriginal was a hero who disarmed the would-be robber. This time, it was students who had scored lowest in racial prejudice who persisted in their reliance on false information, in spite of any attempt at correction. In their subsequent recollections, they mentioned race more frequently, and incorrectly, even though they knew that piece of information had been retracted. … 

### Relation to self-identity

> False beliefs, it turns out, have little to do with one’s stated political affiliations and far more to do with **self-identity**: What kind of person am I, and what kind of person do I want to be? All ideologies are similarly affected. [emphasis added]

## Appendix: An Example of Cognitive Dissonance and Retained Beliefs

From http://www.quackwatch.org/01QuackeryRelatedTopics/ideomotor.html

> Some years ago I participated in a test of applied kinesiology at Dr. Wallace Sampson's medical office in Mountain View, California. A team of chiropractors came to demonstrate the procedure. Several physician observers and the chiropractors had agreed that chiropractors would first be free to illustrate applied kinesiology in whatever manner they chose. Afterward, we would try some double-blind tests of their claims.
> 
> The chiropractors presented as their major example a demonstration they believed showed that the human body could respond to the difference between glucose (a "bad" sugar) and fructose (a "good" sugar). The differential sensitivity was a truism among "alternative healers," though there was no scientific warrant for it. The chiropractors had volunteers lie on their backs and raise one arm vertically. They then would put a drop of glucose (in a solution of water) on the volunteer's tongue. The chiropractor then tried to push the volunteer's upraised arm down to a horizontal position while the volunteer tried to resist. In almost every case, the volunteer could not resist. The chiropractors stated the volunteer's body recognized glucose as a "bad" sugar. After the volunteer's mouth was rinsed out and a drop of fructose was placed on the tongue, the volunteer, in just about every test, resisted movement to the horizontal position. The body had recognized fructose as a "good" sugar.
> 
> After lunch a nurse brought us a large number of test tubes, each one coded with a secret number so that we could not tell from the tubes which contained fructose and which contained glucose. The nurse then left the room so that no one in the room during the subsequent testing would consciously know which tubes contained glucose and which fructose. The arm tests were repeated, but this time they were double-blind -- neither the volunteer, the chiropractors, nor the onlookers was aware of whether the solution being applied to the volunteer's tongue was glucose or fructose. As in the morning session, sometimes the volunteers were able to resist and other times they were not. We recorded the code number of the solution on each trial. Then the nurse returned with the key to the code. When we determined which trials involved glucose and which involved fructose, there was no connection between ability to resist and whether the volunteer was given the "good" or the "bad" sugar.
> 
> When these results were announced, the head chiropractor turned to me and said, "You see, that is why we never do double-blind testing anymore. It never works!" At first I thought he was joking. It turned it out he was quite serious. Since he "knew" that applied kinesiology works, and the best scientific method shows that it does not work, then -- in his mind -- there must be something wrong with the scientific method. This is both a form of loopholism as well as an illustration of what I call the plea for special dispensation. Many pseudo- and fringe-scientists often react to the failure of science to confirm their prized beliefs, not by gracefully accepting the possibility that they were wrong, but by arguing that science is defective.

## Appendix: Speculation on Why We Are Attached to Views [Game Theory and Evolutionary Biology]

Why do we have such strong commitment to views? Why are we so emotionally involved in what we believe: finding ourselves angry and upset when our beliefs are threatened? Think of that common agreement to have no religion or politics at the dinner table or in bars because it leads to arguments and even violence.

Here I’m going to set out one possible explanation in the form of an evolutionary sociobiology “just-so” story.

### Commitment problem

Basic idea: emotional “attachment” to our views is a solution to a commitment problem related to credibility with others.

Imagine Abe and Ben are hunter gatherers. Abe says to Ben: I’ve seen lots of nice Moose over that hill.

Should Ben believe Abe and spend a day trekking over the hill to find the Moose? After all Abe may be:

1. Mistaken
    1. Abe could be talking out of his ass (he just says stuff like this all the time)
    2. Yes, he has some evidence but not a lot 
2. Lying

If “talk is cheap” then Ben probably should not believe Abe. After all, Abe’s claims are not very “credible” — one of those three options is likely correct.

However, suppose instead that talk is not cheap: that Ben knows that Abe really likes to be right. That he gets upset and angry when he is wrong, or, that he gets really ashamed, guilty and stressed. If so, this really adds weight to Abe’s claim. After all, if Ben does go over the hill and finds no moose there will be an emotional cost for Abe.

Overall, this makes Ben more likely to believe Abe.

This emotional cost to being wrong — and payoff to being right — will not be limited just to the locations of moose. It will naturally spread to include all beliefs and opinions we set out (even to ourselves).

This will quickly become a barrier to updating one’s views and opinions if it is associated with “being wrong”. Such commitments to one’s pre-existing views and opinions will even spread to abstract ideas like a belief in God. It may become so emotionally welded to our sense of identity and self that it is strong enough that we will kill for our views and opinions — and admire those who do.

Furthermore, such strong feelings create problems for changing our views especially when combined with the inevitable uncertainty of life. To go back to our story: perhaps the moose are not always in the same place, so the moose being absent when Ben goes over the hill does not necessarily mean that Abe was mistaken and Abe has a strong reason to interpret the data that way. With uncertainty we now we have a valid reason to ignore and even discard new data points that are inconsistent with our pre-existing beliefs.

### Regarding lying

The above explanation talks more to option (A) case of genuine mistake. However, the logic also applies in the lying case.

Lying would also potentially have evolutionary benefits — Abe wants Ben to go to the wrong place so that Abe gets food and Ben does not etc. However, to lie well requires you to be credible. Once again either having or being able to fake a real emotional cost to being wrong is valuable. In addition, with lying the ability to deceive yourself is very valuable — the best liars are the ones who believe their own lies. Thus, the lying option would also encourage a mechanism for deceiving ourselves. This mechanism would contribute even more to our tendency to ignore or dismiss evidence which contradicts our pre-existing views.

#### Background: Commitment Problems

Suppose two robbers Abe and Ben rob a jewellery store. After the robbery they need to split up to reduce their risk of being caught. In addition, it is best if only one of them looks after the loot. In addition, Abe has all the contacts needed to sell it.

However, Ben is worried that if Abe has the loot then he may just run off it with himself. What can they do?

The issue here is what game theorists like to call a commitment problem: at the moment just after the robbery the best thing is for them to split up and only Abe to hold on to the loot. But once they have done that, Abe has an incentive to make off with all of it himself. If they cannot solve this issue they may be forced to do something less optimal: split up the proceeds right now even though that results in a worse outcome — in this case, more risk and a lower price.

What is really wanted here is way for Abe to make a credible commitment to Ben that he will not run off. This is good for both Abe and Ben — they will both end up with less risk and get more money.

A credible commitment for Abe needs to involve something with a greater cost than the benefit of ripping-off Ben and taking all the loot for himself. One option might be that Abe’s daughter is married to Ben’s son. In that case, ripping Ben off will cause huge harm to his family.

Another option might be that Abe knows Ben is a crazy guy who will stop at nothing if he is taken advantage of, that Ben would track him down and kill him — even if that meant risking huge jail time — just to get even if Abe ripped him off.

Though it might sound bad, Ben being crazy in this sense would actually be good for Abe as it would provide a way for Abe to make a credible promise to Ben — Ben would know that Abe knew that he is a crazy guy and would trust Abe not to rip him off.

*The other classic example is nuclear weapons. Remember Dr Strangelove and the Russian’s Doomsday Machine.*